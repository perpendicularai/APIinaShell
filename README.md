# APIinaShell
An API deployed using fastapi running Ollama as the backend to do LLM inference.

## How to
To start using the api, you need to ensure that Ollama is running and being served at the address configured in the script. This could be any address.
- In a command prompt, run `py` or `python` `apiinashell.py`.
This should start the api server at the configured address.
- Open a browser and browse to the address displayed in the command prompt. You should the api interface.
- You can then enter a string to test it out.
