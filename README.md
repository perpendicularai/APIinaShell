# APIinaShell
An API deployed using fastapi running Ollama as the backend to do LLM inference.

## How to
To start using the api, you need to ensure that Ollama is running and being served at the address configured in the script. This could be any address. If you have not installed Ollama, see 
- In a command prompt, run `py` or `python` `apiinashell.py`.
This should start the api server at the configured address.
- Open a browser and browse to the address displayed in the command prompt. You should see the api interface.
- You can then enter a string to test it out. See video below :
`https://github.com/perpendicularai/APIinaShell/assets/146530480/87491a67-4691-4574-90ae-ed55d4126b58`

