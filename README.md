# APIinaShell
An API deployed using fastapi running Ollama as the backend to do LLM inference.
